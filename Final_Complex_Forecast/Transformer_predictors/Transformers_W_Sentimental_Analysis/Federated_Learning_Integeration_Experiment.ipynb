{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52vLvchHB0K-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71cbd3a9-d6ff-42c8-9cff-df7ec3032a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error or no articles found. Response:\n",
            "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n",
            "Error or no articles found. Response:\n",
            "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n",
            "Error or no articles found. Response:\n",
            "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n",
            "Error or no articles found. Response:\n",
            "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n",
            "Error or no articles found. Response:\n",
            "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n",
            "Error or no articles found. Response:\n",
            "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_news_data(query):\n",
        "    api_key = '23f5208b7e2940aeb8b72c67195565b2'\n",
        "    date_from = '2024-01-02'\n",
        "    date_to = '2024-01-31'\n",
        "    sources = 'bloomberg,cnbc,reuters,financial-times,techcrunch,the-wall-street-journal,the-verge,business-insider,the-economist,wired,engadget,bbc-news,fortune,techradar'\n",
        "\n",
        "    url = f'https://newsapi.org/v2/everything?q={query}&from={date_from}&to={date_to}&sources={sources}&apiKey={api_key}'\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if 'articles' in data:\n",
        "        return [(article['title'], article['publishedAt'][:10]) for article in data['articles']]\n",
        "    else:\n",
        "        print(\"Error or no articles found. Response:\")\n",
        "        print(data)\n",
        "        return []\n",
        "\n",
        "# Broadening the search query\n",
        "msft_data = fetch_news_data('Microsoft OR \"MSFT\" OR \"Satya Nadella\" OR \"Windows\" OR \"Azure\" OR \"Office 365\" OR \"Xbox\" OR \"Surface\" OR \"Microsoft Teams\" OR \"LinkedIn\"')\n",
        "\n",
        "ibm_data = fetch_news_data('IBM OR \"International Business Machines\" OR \"Arvind Krishna\" OR \"Watson\" OR \"Cloud computing\" OR \"Quantum computing\" OR \"IBM Research\" OR \"Artificial Intelligence\" OR \"Blockchain\"')\n",
        "\n",
        "amzn_data = fetch_news_data('Amazon OR \"AMZN\" OR \"Jeff Bezos\" OR \"Andy Jassy\" OR \"E-commerce\" OR \"Amazon Web Services\" OR \"AWS\" OR \"Amazon Prime\" OR \"Amazon marketplace\" OR \"Whole Foods\" OR \"Amazon logistics\"')\n",
        "\n",
        "goog_data = fetch_news_data('Google OR \"Alphabet Inc.\" OR \"Sundar Pichai\" OR \"Google Search\" OR \"Android\" OR \"Stocks\" OR \"Google Cloud\" OR \"YouTube\"  OR \"Technology\" OR \"Google Ads\" OR \"Google services\" OR \"Google AI\" OR \"Pixel\" OR \"AI\" OR \"Artificial Intelligence\" OR \"Regulation\" OR \"Google Sustainability\" OR \"Google Health\" OR \"DeepMind\" OR \"Google Quantum Computing\" OR \"Google X\" OR \"Moonshot Factory\"')\n",
        "\n",
        "orcl_data = fetch_news_data('Oracle OR \"ORCL\" OR \"Larry Ellison\" OR \"Safra Catz\" OR \"Enterprise software\" OR \"Cloud services\" OR \"Database technology\" OR \"Oracle financials\" OR \"Technology\"')\n",
        "\n",
        "nvda_data = fetch_news_data('NVDA OR \"NVIDIA Corporation\" OR \"Graphics Processing Units\" OR \"Jensen Huang\" '\n",
        "         'OR \"Artificial Intelligence\" OR \"Gaming Industry\" OR \"Computer Graphics\" '\n",
        "         'OR \"NVIDIA products\" OR \"NVIDIA stock\" OR \"Semiconductor industry\" '\n",
        "         'OR \"Deep learning\" OR \"GPU technology\" OR \"NVIDIA partnerships\" '\n",
        "         'OR \"NVIDIA competition\" OR \"NVIDIA financials\" OR \"Tech industry\" '\n",
        "         'OR \"Data centers\" OR \"Autonomous vehicles\" OR \"Gaming GPUs\" '\n",
        "         'OR \"NVIDIA acquisitions\" OR \"Ray tracing technology\"')\n",
        "\n",
        "fdx_data = fetch_news_data('FedEx OR \"FDX\" OR \"Frederick W. Smith\" OR \"Express delivery\" OR \"Courier services\" '\n",
        "         'OR \"Logistics services\" OR \"FedEx Ground\" OR \"FedEx Freight\" OR \"Shipping industry\" '\n",
        "         'OR \"Parcel delivery\" OR \"E-commerce delivery\" OR \"Supply chain\" OR \"FedEx logistics\" '\n",
        "         'OR \"FedEx innovations\" OR \"Delivery technology\" OR \"FedEx and ecommerce\" '\n",
        "         'OR \"FedEx sustainability\" OR \"Delivery drones\" OR \"FedEx automation\"')\n",
        "\n",
        "abnb_data = fetch_news_data('Airbnb OR \"ABNB\" OR \"Short-term rental\" OR \"Brian Chesky\" OR \"Vacation rental\" '\n",
        "         'OR \"Airbnb experiences\" OR \"Airbnb IPO\" OR \"Home sharing\" OR \"Travel industry\" '\n",
        "         'OR \"Airbnb regulations\" OR \"Hospitality industry\" OR \"Airbnb market\" OR \"Platform economy\" '\n",
        "         'OR \"Sharing economy\" OR \"Airbnb competitors\" OR \"Airbnb legal\" OR \"Airbnb challenges\" '\n",
        "         'OR \"Tourism disruption\" OR \"Airbnb strategy\" OR \"Airbnb expansion\" OR \"Peer-to-peer lodging\" '\n",
        "         'OR \"technology\" OR \"istings\"')\n",
        "\n",
        "tsla_data = fetch_news_data('Elon Musk OR Tesla OR \"Electric Vehicles\" OR \"SpaceX\" OR \"Neuralink\" OR \"Twitter acquisition\" OR \"Tesla stock\" OR \"Autonomous driving\" OR \"Battery technology\" OR \"Solar energy\" OR \"Hyperloop\" OR \"Mars colonization\"')\n",
        "\n",
        "appl_data = fetch_news_data('Apple OR \"Apple Inc.\" OR \"iPhone\" OR \"iPad\" OR \"Mac\" OR \"Apple Watch\" OR \"iOS\" OR \"App Store\" OR \"MacOS\" OR \"Apple Music\" OR \"Tim Cook\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "def analyze_sentiment(news_data):\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "    # Extract just the headlines\n",
        "    headlines = [headline for headline, _ in news_data]\n",
        "\n",
        "    # Tokenize and encode headlines\n",
        "    max_length = 512\n",
        "    inputs = tokenizer(headlines, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "    # Predict sentiment\n",
        "    sentiment_data = []\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Pair each sentiment score with its corresponding date\n",
        "        for (headline, date), prediction in zip(news_data, predictions):\n",
        "            sentiment_data.append({\n",
        "                'date': date,\n",
        "                'headline': headline,\n",
        "                'sentiment_positive': prediction[0].item(),\n",
        "                'sentiment_neutral': prediction[1].item(),\n",
        "                'sentiment_negative': prediction[2].item()\n",
        "            })\n",
        "\n",
        "    return sentiment_data"
      ],
      "metadata": {
        "id": "Evsfb6fhFeUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing sentiment for each company\n",
        "msft_sentiment = analyze_sentiment(msft_data)\n",
        "ibm_sentiment = analyze_sentiment(ibm_data)\n",
        "amzn_sentiment = analyze_sentiment(amzn_data)\n",
        "goog_sentiment = analyze_sentiment(goog_data)\n",
        "orcl_sentiment = analyze_sentiment(orcl_data)\n",
        "nvda_sentiment = analyze_sentiment(nvda_data)\n",
        "fdx_sentiment = analyze_sentiment(fdx_data)\n",
        "abnb_sentiment = analyze_sentiment(abnb_data)\n",
        "tsla_sentiment = analyze_sentiment(tsla_data)\n",
        "appl_sentiment = analyze_sentiment(appl_data)"
      ],
      "metadata": {
        "id": "87zcWCdPFf8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def process_stock_sentiment(sentiment_data, ticker_symbol, start_date, end_date):\n",
        "    # Convert sentiment_data to a DataFrame\n",
        "    sentiment_df = pd.DataFrame(sentiment_data)\n",
        "\n",
        "    # Convert date to datetime and set as index\n",
        "    sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
        "    sentiment_df.set_index('date', inplace=True)\n",
        "\n",
        "    # Aggregate sentiment scores by date\n",
        "    average_sentiment = sentiment_df.groupby('date').mean()\n",
        "\n",
        "    # Download stock data\n",
        "    stock_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
        "\n",
        "    # Selecting the required columns\n",
        "    stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
        "\n",
        "    # Convert stock_data index to datetime\n",
        "    stock_data.index = pd.to_datetime(stock_data.index)\n",
        "\n",
        "    # Combine stock data with sentiment data\n",
        "    combined_data = stock_data.join(average_sentiment)\n",
        "\n",
        "    # Interpolate missing values\n",
        "    combined_data = combined_data.interpolate(method='time')\n",
        "\n",
        "    # Scale price-related columns\n",
        "    price_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
        "    scaler_prices = StandardScaler()\n",
        "    combined_data[price_cols] = scaler_prices.fit_transform(combined_data[price_cols])\n",
        "\n",
        "    # Scale Volume column independently\n",
        "    scaler_volume = StandardScaler()\n",
        "    combined_data['Volume'] = scaler_volume.fit_transform(combined_data[['Volume']])\n",
        "\n",
        "    return combined_data, scaler_prices\n",
        "\n",
        "# Process Microsoft's sentiment and stock data\n",
        "msft_combined_data = process_stock_sentiment(msft_sentiment, 'MSFT', '2024-01-01', '2024-01-30')\n",
        "\n",
        "ibm_combined_data = process_stock_sentiment(ibm_sentiment, 'IBM', '2024-01-01', '2024-01-30')\n",
        "\n",
        "amzn_combined_data = process_stock_sentiment(amzn_sentiment, 'AMZN', '2024-01-01', '2024-01-30')\n",
        "\n",
        "goog_combined_data = process_stock_sentiment(goog_sentiment, 'GOOG', '2024-01-01', '2024-01-30')\n",
        "\n",
        "orcl_combined_data = process_stock_sentiment(orcl_sentiment, 'ORCL', '2024-01-01', '2024-01-30')\n",
        "\n",
        "nvda_combined_data = process_stock_sentiment(nvda_sentiment, 'NVDA', '2024-01-01', '2024-01-30')\n",
        "\n",
        "fdx_combined_data = process_stock_sentiment(fdx_sentiment, 'FDX', '2024-01-01', '2024-01-30')\n",
        "\n",
        "abnb_combined_data = process_stock_sentiment(abnb_sentiment, 'ABNB', '2024-01-01', '2024-01-30')\n",
        "\n",
        "tsla_combined_data = process_stock_sentiment(tsla_sentiment, 'TSLA', '2024-01-01', '2024-01-30')\n",
        "\n",
        "appl_combined_data = process_stock_sentiment(appl_sentiment, 'AAPL', '2024-01-01', '2024-01-30')"
      ],
      "metadata": {
        "id": "SZina0LwFvyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Data for Every Company (Just Checking DataFrames):"
      ],
      "metadata": {
        "id": "8YBxJD59HRwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking to see if there's any NULL"
      ],
      "metadata": {
        "id": "BqrV2kppHz_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msft_combined_data[0]"
      ],
      "metadata": {
        "id": "WS2W22W1HWaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ibm_combined_data[0]"
      ],
      "metadata": {
        "id": "bLFJeinnHY09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amzn_combined_data[0]"
      ],
      "metadata": {
        "id": "wYO-INE9HgBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goog_combined_data[0]"
      ],
      "metadata": {
        "id": "4YjyBhsJHaJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orcl_combined_data[0]"
      ],
      "metadata": {
        "id": "O9tXq4rSHhlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nvda_combined_data[0]"
      ],
      "metadata": {
        "id": "G1ab5lmUHj6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdx_combined_data[0]"
      ],
      "metadata": {
        "id": "q6VmsnzDHo4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abnb_combined_data[0]"
      ],
      "metadata": {
        "id": "Qb8PzR-ZHqbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsla_combined_data[0]"
      ],
      "metadata": {
        "id": "xkceMZeBHwv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "appl_combined_data[0]"
      ],
      "metadata": {
        "id": "X8XUIVPBHPjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing data into model"
      ],
      "metadata": {
        "id": "2uNWinnkH8u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, MultiHeadAttention, GlobalAveragePooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def transformer_model(combined_data, scaler_prices, company_name):\n",
        "  selected_features = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'sentiment_positive', 'sentiment_neutral', 'sentiment_negative']\n",
        "  data = combined_data[selected_features]\n",
        "\n",
        "  train_size = int(len(data) * 0.7)\n",
        "  val_size = int(len(data) * 0.15)\n",
        "  train_data = data.iloc[:train_size]\n",
        "  val_data = data.iloc[train_size:train_size + val_size]\n",
        "  test_data = data.iloc[train_size + val_size:]\n",
        "\n",
        "  # Prepare features and target for model training\n",
        "  X_train = train_data.drop('Close', axis=1)\n",
        "  y_train = train_data['Close']\n",
        "  X_val = val_data.drop('Close', axis=1)\n",
        "  y_val = val_data['Close']\n",
        "  X_test = test_data.drop('Close', axis=1)\n",
        "  y_test = test_data['Close']\n",
        "\n",
        "  # Define and compile the neural network model\n",
        "  def create_model(input_shape):\n",
        "      inp = Input(shape=input_shape)\n",
        "\n",
        "      # LSTM Layer\n",
        "      x = LSTM(128, return_sequences=True)(inp)\n",
        "      x = Dropout(0.2)(x)\n",
        "\n",
        "      # Transformer Layer using MultiHeadAttention\n",
        "      attention = MultiHeadAttention(num_heads=2, key_dim=128)(x, x)\n",
        "      x = GlobalAveragePooling1D()(attention)\n",
        "\n",
        "      # Dense Layers for final predictions\n",
        "      x = Dense(64, activation='LeakyReLU')(x)\n",
        "      x = Dropout(0.2)(x)\n",
        "      x = Dense(32, activation='LeakyReLU')(x)\n",
        "      x = Dense(1)(x)\n",
        "\n",
        "      model = Model(inputs=inp, outputs=x)\n",
        "      model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "      return model\n",
        "\n",
        "  # Reshape the data for LSTM layer\n",
        "  X_train_reshaped = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "  X_val_reshaped = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
        "  X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "  # Train the model\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
        "\n",
        "  # Train the model with early stopping\n",
        "  model = create_model((1, X_train.shape[1]))\n",
        "  history = model.fit(X_train_reshaped, y_train, epochs=50, batch_size=8, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
        "\n",
        "  train_loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "  # Convert the epochs range object to a list\n",
        "  epochs_list = list(epochs)\n",
        "\n",
        "  # Plotting the training and validation loss\n",
        "  loss_fig = go.Figure()\n",
        "  loss_fig.add_trace(go.Scatter(x=epochs_list, y=train_loss, mode='lines', name=f'{company_name} Training Loss'))\n",
        "  loss_fig.add_trace(go.Scatter(x=epochs_list, y=val_loss, mode='lines', name=f'{company_name} Validation Loss'))\n",
        "  loss_fig.update_layout(title=f'{company_name} Training and Validation Loss per Epoch',\n",
        "                      xaxis_title='Epochs',\n",
        "                      yaxis_title='Loss',\n",
        "                      legend_title='Type')\n",
        "  loss_fig.show()\n",
        "\n",
        "  # Make predictions\n",
        "  # Reshape X_test for prediction\n",
        "  X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "  # Make predictions using the reshaped test data\n",
        "  predictions = model.predict(X_test_reshaped)\n",
        "\n",
        "  # Evaluate the model\n",
        "  mse = mean_squared_error(y_test, predictions)\n",
        "  mae = mean_absolute_error(y_test, predictions)\n",
        "  r2 = r2_score(y_test, predictions)\n",
        "  print(f'MSE: {mse}, MAE: {mae}, R^2: {r2}')\n",
        "\n",
        "  # Reshape y_test and predictions for inverse transform\n",
        "  y_test_reshaped = y_test.values.reshape(-1, 1)\n",
        "  predictions_reshaped = predictions.reshape(-1, 1)\n",
        "\n",
        "  num_scaled_cols = 5\n",
        "\n",
        "  # Create separate dummy arrays for inverse scaling\n",
        "  dummy_array_y_test = np.zeros((len(y_test_reshaped), num_scaled_cols))\n",
        "  dummy_array_predictions = np.zeros((len(predictions_reshaped), num_scaled_cols))\n",
        "\n",
        "  # Fill in the 'Close' column values in the dummy arrays\n",
        "  # Assuming 'Close' is the last of the scaled columns\n",
        "  dummy_array_y_test[:, -1] = y_test_reshaped.flatten()\n",
        "  dummy_array_predictions[:, -1] = predictions_reshaped.flatten()\n",
        "\n",
        "  # Inverse transform the 'Close' prices using the dummy arrays\n",
        "  y_test_original = scaler_prices.inverse_transform(dummy_array_y_test)[:, -1]\n",
        "  predictions_original = scaler_prices.inverse_transform(dummy_array_predictions)[:, -1]\n",
        "\n",
        "  # Extracting testing dates\n",
        "  test_dates = test_data.index\n",
        "\n",
        "  # Plotting with Plotly\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=test_dates, y=y_test_original, mode='lines', name=f'Actual {company_name}'))\n",
        "  fig.add_trace(go.Scatter(x=test_dates, y=predictions_original, mode='lines', name=f'Predicted {company_name}'))\n",
        "  fig.update_layout(title=f'Actual vs Predicted {company_name} Stock Prices',\n",
        "                    xaxis_title='Date',\n",
        "                    yaxis_title='Stock Price',\n",
        "                    legend_title='Legend')\n",
        "  fig.show()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "gcbV7ChSH8BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots from models:"
      ],
      "metadata": {
        "id": "O9LPUOVDKe1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msft_model = transformer_model(msft_combined_data[0], msft_combined_data[1], 'MSFT')\n",
        "\n",
        "msft_model"
      ],
      "metadata": {
        "id": "OQ4gG7nGKi_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ibm_model = transformer_model(ibm_combined_data[0], ibm_combined_data[1], 'IBM')\n",
        "\n",
        "ibm_model"
      ],
      "metadata": {
        "id": "cfctEWTyKkIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amzn_model = transformer_model(amzn_combined_data[0], amzn_combined_data[1], 'AMZN')\n",
        "\n",
        "amzn_model"
      ],
      "metadata": {
        "id": "bjdiBugbK1HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goog_model = transformer_model(goog_combined_data[0], goog_combined_data[1], 'GOOG')\n",
        "\n",
        "goog_model"
      ],
      "metadata": {
        "id": "pUWLlxkdK7BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orcl_model = transformer_model(orcl_combined_data[0], orcl_combined_data[1], 'ORCL')\n",
        "\n",
        "orcl_model"
      ],
      "metadata": {
        "id": "6FmNZbklLDCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nvda_model = transformer_model(nvda_combined_data[0], nvda_combined_data[1], 'NVDA')\n",
        "\n",
        "nvda_model"
      ],
      "metadata": {
        "id": "-Zs4_lbuLI02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdx_model = transformer_model(fdx_combined_data[0], fdx_combined_data[1], 'FDX')\n",
        "\n",
        "fdx_model"
      ],
      "metadata": {
        "id": "-7o7VLHXLNC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abnb_model = transformer_model(abnb_combined_data[0], abnb_combined_data[1], 'ABNB')\n",
        "\n",
        "abnb_model"
      ],
      "metadata": {
        "id": "HqdfBbhKLSaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsla_model = transformer_model(tsla_combined_data[0], tsla_combined_data[1], 'TSLA')\n",
        "\n",
        "tsla_model"
      ],
      "metadata": {
        "id": "ArrYLnBaLXV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "appl_model = transformer_model(appl_combined_data[0], appl_combined_data[1], 'APPL')\n",
        "\n",
        "appl_model"
      ],
      "metadata": {
        "id": "Jdvs7wGhLdf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Federated Learning"
      ],
      "metadata": {
        "id": "TmvfP9FbB6gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you have these models already trained\n",
        "existing_models = [msft_model, ibm_model, amzn_model, goog_model, orcl_model, nvda_model, fdx_model, abnb_model, tsla_model, appl_model]\n",
        "\n",
        "def average_model_weights(models):\n",
        "    # Filter out any None values from the models list\n",
        "    valid_models = [model for model in models if model is not None]\n",
        "\n",
        "    if not valid_models:\n",
        "        raise ValueError(\"No valid models provided for averaging weights.\")\n",
        "\n",
        "    num_layers = len(valid_models[0].layers)\n",
        "    average_weights = []\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        layer_weights = np.array([model.layers[layer].get_weights() for model in valid_models if model.layers[layer].get_weights()])\n",
        "\n",
        "        if len(layer_weights) > 0:\n",
        "            # Calculate the mean across the first axis, which represents each model's layer weights\n",
        "            avg_layer_weights = np.mean(layer_weights, axis=0)\n",
        "            average_weights.append(avg_layer_weights)\n",
        "\n",
        "    # Flatten the list as the set_weights function expects a flat list of arrays\n",
        "    average_weights_flat = [weight for layer in average_weights for weight in layer]\n",
        "    return average_weights_flat\n",
        "\n",
        "avg_weights = average_model_weights(existing_models)\n",
        "\n",
        "def transformer_avg_model(combined_data, scaler_prices, company_name, avg_weights):\n",
        "  selected_features = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'sentiment_positive', 'sentiment_neutral', 'sentiment_negative']\n",
        "  data = combined_data[selected_features]\n",
        "\n",
        "  train_size = int(len(data) * 0.7)\n",
        "  val_size = int(len(data) * 0.15)\n",
        "  train_data = data.iloc[:train_size]\n",
        "  val_data = data.iloc[train_size:train_size + val_size]\n",
        "  test_data = data.iloc[train_size + val_size:]\n",
        "\n",
        "  # Prepare features and target for model training\n",
        "  X_train = train_data.drop('Close', axis=1)\n",
        "  y_train = train_data['Close']\n",
        "  X_val = val_data.drop('Close', axis=1)\n",
        "  y_val = val_data['Close']\n",
        "  X_test = test_data.drop('Close', axis=1)\n",
        "  y_test = test_data['Close']\n",
        "\n",
        "  # Define and compile the neural network model\n",
        "  def create_model(input_shape):\n",
        "      inp = Input(shape=input_shape)\n",
        "\n",
        "      # LSTM Layer\n",
        "      x = LSTM(128, return_sequences=True)(inp)\n",
        "      x = Dropout(0.2)(x)\n",
        "\n",
        "      # Transformer Layer using MultiHeadAttention\n",
        "      attention = MultiHeadAttention(num_heads=2, key_dim=128)(x, x)\n",
        "      x = GlobalAveragePooling1D()(attention)\n",
        "\n",
        "      # Dense Layers for final predictions\n",
        "      x = Dense(64, activation='LeakyReLU')(x)\n",
        "      x = Dropout(0.2)(x)\n",
        "      x = Dense(32, activation='LeakyReLU')(x)\n",
        "      x = Dense(1)(x)\n",
        "\n",
        "      model = Model(inputs=inp, outputs=x)\n",
        "      model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "      return model\n",
        "\n",
        "  # Reshape the data for LSTM layer\n",
        "  X_train_reshaped = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "  X_val_reshaped = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
        "  X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "  # Train the model\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
        "\n",
        "  # Train the model with early stopping\n",
        "  model = create_model((1, X_train.shape[1]))\n",
        "  model.set_weights(avg_weights)\n",
        "  history = model.fit(X_train_reshaped, y_train, epochs=50, batch_size=8, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
        "\n",
        "  train_loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "  # Convert the epochs range object to a list\n",
        "  epochs_list = list(epochs)\n",
        "\n",
        "  # Plotting the training and validation loss\n",
        "  loss_fig = go.Figure()\n",
        "  loss_fig.add_trace(go.Scatter(x=epochs_list, y=train_loss, mode='lines', name=f'{company_name} Training Loss'))\n",
        "  loss_fig.add_trace(go.Scatter(x=epochs_list, y=val_loss, mode='lines', name=f'{company_name} Validation Loss'))\n",
        "  loss_fig.update_layout(title=f'{company_name} Training and Validation Loss per Epoch',\n",
        "                      xaxis_title='Epochs',\n",
        "                      yaxis_title='Loss',\n",
        "                      legend_title='Type')\n",
        "  loss_fig.show()\n",
        "\n",
        "  # Make predictions\n",
        "  # Reshape X_test for prediction\n",
        "  X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "  # Make predictions using the reshaped test data\n",
        "  predictions = model.predict(X_test_reshaped)\n",
        "\n",
        "  # Evaluate the model\n",
        "  mse = mean_squared_error(y_test, predictions)\n",
        "  mae = mean_absolute_error(y_test, predictions)\n",
        "  r2 = r2_score(y_test, predictions)\n",
        "  print(f'MSE: {mse}, MAE: {mae}, R^2: {r2}')\n",
        "\n",
        "  # Reshape y_test and predictions for inverse transform\n",
        "  y_test_reshaped = y_test.values.reshape(-1, 1)\n",
        "  predictions_reshaped = predictions.reshape(-1, 1)\n",
        "\n",
        "  num_scaled_cols = 5\n",
        "\n",
        "  # Create separate dummy arrays for inverse scaling\n",
        "  dummy_array_y_test = np.zeros((len(y_test_reshaped), num_scaled_cols))\n",
        "  dummy_array_predictions = np.zeros((len(predictions_reshaped), num_scaled_cols))\n",
        "\n",
        "  # Fill in the 'Close' column values in the dummy arrays\n",
        "  # Assuming 'Close' is the last of the scaled columns\n",
        "  dummy_array_y_test[:, -1] = y_test_reshaped.flatten()\n",
        "  dummy_array_predictions[:, -1] = predictions_reshaped.flatten()\n",
        "\n",
        "  # Inverse transform the 'Close' prices using the dummy arrays\n",
        "  y_test_original = scaler_prices.inverse_transform(dummy_array_y_test)[:, -1]\n",
        "  predictions_original = scaler_prices.inverse_transform(dummy_array_predictions)[:, -1]\n",
        "\n",
        "  # Extracting testing dates\n",
        "  test_dates = test_data.index\n",
        "\n",
        "  # Plotting with Plotly\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=test_dates, y=y_test_original, mode='lines', name=f'Actual {company_name}'))\n",
        "  fig.add_trace(go.Scatter(x=test_dates, y=predictions_original, mode='lines', name=f'Predicted {company_name}'))\n",
        "  fig.update_layout(title=f'Actual vs Predicted {company_name} Stock Prices',\n",
        "                    xaxis_title='Date',\n",
        "                    yaxis_title='Stock Price',\n",
        "                    legend_title='Legend')\n",
        "  fig.show()\n",
        "\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "mQBIbwILB8GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msft_transformed_model = transformer_avg_model(msft_combined_data[0], msft_combined_data[1], 'MSFT', avg_weights)\n",
        "ibm_transformed_model = transformer_avg_model(ibm_combined_data[0], ibm_combined_data[1], 'IBM', avg_weights)\n",
        "amzn_transformed_model = transformer_avg_model(amzn_combined_data[0], amzn_combined_data[1], 'AMZN', avg_weights)\n",
        "goog_transformed_model = transformer_avg_model(goog_combined_data[0], goog_combined_data[1], 'GOOG', avg_weights)\n",
        "orcl_transformed_model = transformer_avg_model(orcl_combined_data[0], orcl_combined_data[1], 'ORCL', avg_weights)\n",
        "nvda_transformed_model = transformer_avg_model(nvda_combined_data[0], nvda_combined_data[1], 'NVDA', avg_weights)\n",
        "fdx_transformed_model = transformer_avg_model(fdx_combined_data[0], fdx_combined_data[1], 'FDX', avg_weights)\n",
        "abnb_transformed_model = transformer_avg_model(abnb_combined_data[0], abnb_combined_data[1], 'ABNB', avg_weights)\n",
        "tsla_transformed_model = transformer_avg_model(tsla_combined_data[0], tsla_combined_data[1], 'TSLA', avg_weights)\n",
        "appl_transformed_model = transformer_avg_model(appl_combined_data[0], appl_combined_data[1], 'APPL', avg_weights)"
      ],
      "metadata": {
        "id": "6FJ6NVUoaoAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import plotly.graph_objects as go\n",
        "\n",
        "# def calculate_r_squared(model, x, y):\n",
        "#     from sklearn.metrics import r2_score\n",
        "\n",
        "#     predictions = model.predict(x)\n",
        "#     r_squared = r2_score(y, predictions)\n",
        "#     return r_squared\n",
        "\n",
        "# # Configuration\n",
        "# num_iterations = 20\n",
        "# r_squared_threshold = 0.75  # Define your R-squared threshold\n",
        "# company_names = ['MSFT', 'IBM', 'AMZN', 'GOOG', 'ORCL', 'NVDA', 'FDX', 'ABNB', 'TSLA', 'APPL']\n",
        "# company_data = [None] * len(company_names)  # Replace with your actual data\n",
        "# group_a_losses = {'train': [], 'val': []}\n",
        "# group_b_losses = {'train': [], 'val': []}\n",
        "\n",
        "# for iteration in range(num_iterations):\n",
        "#     group_a_models, group_b_models = [], []\n",
        "\n",
        "#     for company_name, data in zip(company_names, company_data):\n",
        "#         model, history, r_squared = transformer_avg_model(company_data[0], company_data[1], company_name, None)\n",
        "\n",
        "#         if r_squared >= r_squared_threshold:\n",
        "#             group_a_models.append((company_name, model))\n",
        "#         else:\n",
        "#             group_b_models.append((company_name, model))\n",
        "\n",
        "#     for group, models in [('A', group_a_models), ('B', group_b_models)]:\n",
        "#         avg_weights = average_model_weights([model for _, model in models])\n",
        "#         iteration_train_losses = []\n",
        "#         iteration_val_losses = []\n",
        "\n",
        "#         for company_name, model in models:\n",
        "#             _, history = transformer_avg_model(data[0], data[1], company_name, avg_weights)  # Retrain with average weights\n",
        "\n",
        "#             iteration_train_losses.extend(history['loss'])\n",
        "#             iteration_val_losses.extend(history['val_loss'])\n",
        "\n",
        "#         avg_train_loss = np.mean(iteration_train_losses)\n",
        "#         avg_val_loss = np.mean(iteration_val_losses)\n",
        "\n",
        "#         if group == 'A':\n",
        "#             group_a_losses['train'].append(avg_train_loss)\n",
        "#             group_a_losses['val'].append(avg_val_loss)\n",
        "#         else:  # group == 'B'\n",
        "#             group_b_losses['train'].append(avg_train_loss)\n",
        "#             group_b_losses['val'].append(avg_val_loss)\n",
        "\n",
        "# # Plotting the results using Plotly\n",
        "# fig = go.Figure()\n",
        "\n",
        "# # Adding traces for Group A\n",
        "# fig.add_trace(go.Scatter(\n",
        "#     x=list(range(num_iterations)),\n",
        "#     y=group_a_losses['train'],\n",
        "#     mode='lines+markers',\n",
        "#     name='Group A Average Training Loss'\n",
        "# ))\n",
        "# fig.add_trace(go.Scatter(\n",
        "#     x=list(range(num_iterations)),\n",
        "#     y=group_a_losses['val'],\n",
        "#     mode='lines+markers',\n",
        "#     name='Group A Average Validation Loss'\n",
        "# ))\n",
        "\n",
        "# # Adding traces for Group B\n",
        "# fig.add_trace(go.Scatter(\n",
        "#     x=list(range(num_iterations)),\n",
        "#     y=group_b_losses['train'],\n",
        "#     mode='lines+markers',\n",
        "#     name='Group B Average Training Loss'\n",
        "# ))\n",
        "# fig.add_trace(go.Scatter(\n",
        "#     x=list(range(num_iterations)),\n",
        "#     y=group_b_losses['val'],\n",
        "#     mode='lines+markers',\n",
        "#     name='Group B Average Validation Loss'\n",
        "# ))\n",
        "\n",
        "# # Updating the layout of the figure\n",
        "# fig.update_layout(\n",
        "#     title='Average Training and Validation Loss Across Iterations for Groups A and B',\n",
        "#     xaxis_title='Iteration',\n",
        "#     yaxis_title='Loss',\n",
        "#     legend_title='Legend',\n",
        "#     font=dict(\n",
        "#         family=\"Courier New, monospace\",\n",
        "#         size=12,\n",
        "#         color=\"RebeccaPurple\"\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# # Show the figure\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "AYcv7l1sPP8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_iterations = 10  # Define the number of iterations for the process\n",
        "all_train_losses = []\n",
        "all_val_losses = []\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    # Average the weights\n",
        "    avg_weights = average_model_weights(existing_models)\n",
        "\n",
        "    iteration_train_losses = []\n",
        "    iteration_val_losses = []\n",
        "\n",
        "    # Retrain models for each company\n",
        "    for company_data, company_name in zip([msft_combined_data, ibm_combined_data, amzn_combined_data, goog_combined_data, orcl_combined_data, nvda_combined_data, fdx_combined_data, abnb_combined_data, tsla_combined_data, appl_combined_data], ['MSFT', 'IBM', 'AMZN', 'GOOG', 'ORCL', 'NVDA', 'FDX', 'ABNB', 'TSLA', 'APPL']):\n",
        "        model, history = transformer_avg_model(company_data[0], company_data[1], company_name, avg_weights)\n",
        "\n",
        "        # Record the losses for this iteration\n",
        "        iteration_train_losses.extend(history.history['loss'])\n",
        "        iteration_val_losses.extend(history.history['val_loss'])\n",
        "\n",
        "        # Update the existing_models list with the retrained model\n",
        "        if company_name == 'MSFT':\n",
        "            msft_model = model\n",
        "        elif company_name == 'IBM':\n",
        "            ibm_model = model\n",
        "        elif company_name == 'AMZN':\n",
        "            amzn_model = model\n",
        "        elif company_name == 'GOOG':\n",
        "            goog_model = model\n",
        "        elif company_name == 'ORCL':\n",
        "            orcl_model = model\n",
        "        elif company_name == 'NVDA':\n",
        "            nvda_model = model\n",
        "        elif company_name == 'FDX':\n",
        "            fdx_model = model\n",
        "        elif company_name == 'ABNB':\n",
        "            abnb_model = model\n",
        "        elif company_name == 'TSLA':\n",
        "            tsla_model = model\n",
        "        elif company_name == 'APPL':\n",
        "            appl_model = model\n",
        "\n",
        "    existing_models = [msft_model, ibm_model, amzn_model, goog_model, orcl_model, nvda_model, fdx_model, abnb_model, tsla_model, appl_model]  # Update the list with retrained models\n",
        "\n",
        "    # Calculate average losses for this iteration\n",
        "    avg_train_loss = np.mean(iteration_train_losses)\n",
        "    avg_val_loss = np.mean(iteration_val_losses)\n",
        "\n",
        "    all_train_losses.append(avg_train_loss)\n",
        "    all_val_losses.append(avg_val_loss)\n",
        "\n",
        "    existing_models = [msft_model, ibm_model, amzn_model, goog_model, orcl_model, nvda_model, fdx_model, abnb_model, tsla_model, appl_model]  # Update the list with retrained models\n",
        "\n",
        "\n",
        "# Creating a figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Adding the training loss plot\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=list(range(num_iterations)),\n",
        "    y=all_train_losses,\n",
        "    mode='lines+markers',\n",
        "    name='Average Training Loss'\n",
        "))\n",
        "\n",
        "# Adding the validation loss plot\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=list(range(num_iterations)),\n",
        "    y=all_val_losses,\n",
        "    mode='lines+markers',\n",
        "    name='Average Validation Loss'\n",
        "))\n",
        "\n",
        "# Updating the layout of the figure\n",
        "fig.update_layout(\n",
        "    title='Average Training and Validation Loss Across Iterations',\n",
        "    xaxis_title='Iteration',\n",
        "    yaxis_title='Loss',\n",
        "    legend_title='Legend',\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=12,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dOAKQY5fgytC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Adding the training loss plot\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=list(range(num_iterations)),\n",
        "    y=all_train_losses,\n",
        "    mode='lines+markers',\n",
        "    name='Average Training Loss'\n",
        "))\n",
        "\n",
        "# Adding the validation loss plot\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=list(range(num_iterations)),\n",
        "    y=all_val_losses,\n",
        "    mode='lines+markers',\n",
        "    name='Average Validation Loss'\n",
        "))\n",
        "\n",
        "# Updating the layout of the figure\n",
        "fig.update_layout(\n",
        "    title='Average Training and Validation Loss Across Iterations',\n",
        "    xaxis_title='Iteration',\n",
        "    yaxis_title='Loss',\n",
        "    legend_title='Legend',\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=12,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "bchV4Io38Ngg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbixRVkhp6Kn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
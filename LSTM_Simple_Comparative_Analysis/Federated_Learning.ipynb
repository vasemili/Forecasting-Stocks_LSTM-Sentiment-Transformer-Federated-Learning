{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "from matplotlib.pyplot import figure\n",
    "import keras;\n",
    "import tensorflow as tf;\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator;\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_TSLA = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\TSLA.csv')#, index_col = 'Date')\n",
    "df_AAPL = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\AAPL.csv')#, index_col = 'Date')\n",
    "df_ABNB = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\ABNB.csv')#, index_col = 'Date')\n",
    "df_AMZN = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\AMZN.csv')#, index_col = 'Date')\n",
    "df_BTC = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\BTC-USD.csv')#, index_col = 'Date')\n",
    "df_FDX = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\FDX.csv')#, index_col = 'Date')\n",
    "df_IBM = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\IBM.csv')#, index_col = 'Date')\n",
    "df_MSFT = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\MSFT.csv')#, index_col = 'Date')\n",
    "df_NVDA = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\NVDA.csv')#, index_col = 'Date')\n",
    "df_ORCL = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\ORCL.csv')#, index_col = 'Date')\n",
    "df_GOOG = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\GOOG.csv')#, index_col = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSLA_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\TSLA_future.csv')#, index_col = 'Date')\n",
    "df_AAPL_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\AAPL_future.csv')#, index_col = 'Date')\n",
    "df_ABNB_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\ABNB_future.csv')#, index_col = 'Date')\n",
    "df_AMZN_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\AMZN_future.csv')#, index_col = 'Date')\n",
    "df_BTC_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\BTC-USD_future.csv')#, index_col = 'Date')\n",
    "df_FDX_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\FDX_future.csv')#, index_col = 'Date')\n",
    "df_IBM_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\IBM_future.csv')#, index_col = 'Date')\n",
    "df_MSFT_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\MSFT_future.csv')#, index_col = 'Date')\n",
    "df_NVDA_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\NVDA_future.csv')#, index_col = 'Date')\n",
    "df_ORCL_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\ORCL_future.csv')#, index_col = 'Date')\n",
    "df_GOOG_future = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\GOOG_future.csv')#, index_col = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSLA_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\TSLA_comp.csv')#, index_col = 'Date')\n",
    "df_AAPL_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\AAPL_comp.csv')#, index_col = 'Date')\n",
    "df_ABNB_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\ABNB_comp.csv')#, index_col = 'Date')\n",
    "df_AMZN_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\AMZN_comp.csv')#, index_col = 'Date')\n",
    "df_BTC_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\BTC-USD_comp.csv')#, index_col = 'Date')\n",
    "df_FDX_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\FDX_comp.csv')#, index_col = 'Date')\n",
    "df_IBM_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\IBM_comp.csv')#, index_col = 'Date')\n",
    "df_MSFT_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\MSFT_comp.csv')#, index_col = 'Date')\n",
    "df_NVDA_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\NVDA_comp.csv')#, index_col = 'Date')\n",
    "df_ORCL_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\ORCL_comp.csv')#, index_col = 'Date')\n",
    "df_GOOG_comp = pd.read_csv(r'C:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\GOOG_comp.csv')#, index_col = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_TSLA['Date'] = pd.to_datetime(df_TSLA['Date']);\n",
    "# df_TSLA.set_index('Date', inplace=True)\n",
    "# df_AAPL['Date'] = pd.to_datetime(df_AAPL['Date']);\n",
    "# df_AAPL.set_index(df_AAPL['Date'], inplace=True);\n",
    "# df_ABNB['Date'] = pd.to_datetime(df_ABNB['Date']);\n",
    "# df_ABNB.set_index(df_ABNB['Date'], inplace=True);\n",
    "# df_AMZN['Date'] = pd.to_datetime(df_AMZN['Date']);\n",
    "# f_AMZN.set_index(df_AMZN['Date'], inplace=True);\n",
    "# df_BTC['Date'] = pd.to_datetime(df_BTC['Date']);\n",
    "# df_BTC.set_index(df_BTC['Date'], inplace=True);\n",
    "# df_FDX['Date'] = pd.to_datetime(df_FDX['Date']);\n",
    "# df_FDX.set_index(df_FDX['Date'], inplace=True);\n",
    "# df_IBM['Date'] = pd.to_datetime(df_IBM['Date']);\n",
    "# df_IBM.set_index(df_IBM['Date'], inplace=True);\n",
    "# df_MSFT['Date'] = pd.to_datetime(df_MSFT['Date']);\n",
    "# df_MSFT.set_index(df_MSFT['Date'], inplace=True);\n",
    "# df_NVDA['Date'] = pd.to_datetime(df_NVDA['Date']);\n",
    "# df_NVDA.set_index(df_NVDA['Date'], inplace=True);\n",
    "# df_ORCL['Date'] = pd.to_datetime(df_ORCL['Date']);\n",
    "# df_ORCL.set_index(df_ORCL['Date'], inplace=True);\n",
    "# df_GOOG['Date'] = pd.to_datetime(df_GOOG['Date']);\n",
    "# df_GOOG.set_index(df_GOOG['Date'], inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-06-30</th>\n",
       "      <td>1.719333</td>\n",
       "      <td>2.028000</td>\n",
       "      <td>1.553333</td>\n",
       "      <td>1.588667</td>\n",
       "      <td>1.588667</td>\n",
       "      <td>257806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-01</th>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.728000</td>\n",
       "      <td>1.351333</td>\n",
       "      <td>1.464000</td>\n",
       "      <td>1.464000</td>\n",
       "      <td>123282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-02</th>\n",
       "      <td>1.533333</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>1.247333</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>77097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-06</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.055333</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>103003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-07</th>\n",
       "      <td>1.093333</td>\n",
       "      <td>1.108667</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>103825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>266.149994</td>\n",
       "      <td>275.570007</td>\n",
       "      <td>262.470001</td>\n",
       "      <td>265.250000</td>\n",
       "      <td>265.250000</td>\n",
       "      <td>67726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-03</th>\n",
       "      <td>254.500000</td>\n",
       "      <td>255.160004</td>\n",
       "      <td>241.009995</td>\n",
       "      <td>242.399994</td>\n",
       "      <td>242.399994</td>\n",
       "      <td>98363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-04</th>\n",
       "      <td>250.520004</td>\n",
       "      <td>257.500000</td>\n",
       "      <td>242.009995</td>\n",
       "      <td>249.440002</td>\n",
       "      <td>249.440002</td>\n",
       "      <td>109578500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-05</th>\n",
       "      <td>245.009995</td>\n",
       "      <td>246.669998</td>\n",
       "      <td>233.270004</td>\n",
       "      <td>240.809998</td>\n",
       "      <td>240.809998</td>\n",
       "      <td>86982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-06</th>\n",
       "      <td>239.440002</td>\n",
       "      <td>244.580002</td>\n",
       "      <td>235.350006</td>\n",
       "      <td>238.130005</td>\n",
       "      <td>238.130005</td>\n",
       "      <td>69298400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3090 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2010-06-30    1.719333    2.028000    1.553333    1.588667    1.588667   \n",
       "2010-07-01    1.666667    1.728000    1.351333    1.464000    1.464000   \n",
       "2010-07-02    1.533333    1.540000    1.247333    1.280000    1.280000   \n",
       "2010-07-06    1.333333    1.333333    1.055333    1.074000    1.074000   \n",
       "2010-07-07    1.093333    1.108667    0.998667    1.053333    1.053333   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-09-30  266.149994  275.570007  262.470001  265.250000  265.250000   \n",
       "2022-10-03  254.500000  255.160004  241.009995  242.399994  242.399994   \n",
       "2022-10-04  250.520004  257.500000  242.009995  249.440002  249.440002   \n",
       "2022-10-05  245.009995  246.669998  233.270004  240.809998  240.809998   \n",
       "2022-10-06  239.440002  244.580002  235.350006  238.130005  238.130005   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2010-06-30  257806500  \n",
       "2010-07-01  123282000  \n",
       "2010-07-02   77097000  \n",
       "2010-07-06  103003500  \n",
       "2010-07-07  103825500  \n",
       "...               ...  \n",
       "2022-09-30   67726600  \n",
       "2022-10-03   98363500  \n",
       "2022-10-04  109578500  \n",
       "2022-10-05   86982700  \n",
       "2022-10-06   69298400  \n",
       "\n",
       "[3090 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_TSLA_future['Date'] = pd.to_datetime(df_TSLA_future['Date']);df_TSLA_future.set_axis(df_TSLA_future['Date'], inplace=True);\n",
    "# df_AAPL_future['Date'] = pd.to_datetime(df_AAPL_future['Date']);df_AAPL_future.set_axis(df_AAPL_future['Date'], inplace=True);\n",
    "# df_ABNB_future['Date'] = pd.to_datetime(df_ABNB_future['Date']);df_ABNB_future.set_axis(df_ABNB_future['Date'], inplace=True);\n",
    "# df_AMZN_future['Date'] = pd.to_datetime(df_AMZN_future['Date']);df_AMZN_future.set_axis(df_AMZN_future['Date'], inplace=True);\n",
    "# df_BTC_future['Date'] = pd.to_datetime(df_BTC_future['Date']);df_BTC_future.set_axis(df_BTC_future['Date'], inplace=True);\n",
    "# df_FDX_future['Date'] = pd.to_datetime(df_FDX_future['Date']);df_FDX_future.set_axis(df_FDX_future['Date'], inplace=True);\n",
    "# df_IBM_future['Date'] = pd.to_datetime(df_IBM_future['Date']);df_IBM_future.set_axis(df_IBM_future['Date'], inplace=True);\n",
    "# df_MSFT_future['Date'] = pd.to_datetime(df_MSFT_future['Date']);df_MSFT_future.set_axis(df_MSFT_future['Date'], inplace=True);\n",
    "# df_NVDA_future['Date'] = pd.to_datetime(df_NVDA_future['Date']);df_NVDA_future.set_axis(df_NVDA_future['Date'], inplace=True);\n",
    "# df_ORCL_future['Date'] = pd.to_datetime(df_ORCL_future['Date']);df_ORCL_future.set_axis(df_ORCL_future['Date'], inplace=True);\n",
    "# df_GOOG_future['Date'] = pd.to_datetime(df_GOOG_future['Date']);df_GOOG_future.set_axis(df_GOOG_future['Date'], inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_TSLA_comp['Date'] = pd.to_datetime(df_TSLA_comp['Date']);df_TSLA_comp.set_axis(df_TSLA_comp['Date'], inplace=True);\n",
    "# df_AAPL_comp['Date'] = pd.to_datetime(df_AAPL_comp['Date']);df_AAPL_comp.set_axis(df_AAPL_comp['Date'], inplace=True);\n",
    "# df_ABNB_comp['Date'] = pd.to_datetime(df_ABNB_comp['Date']);df_ABNB_comp.set_axis(df_ABNB_comp['Date'], inplace=True);\n",
    "# df_AMZN_comp['Date'] = pd.to_datetime(df_AMZN_comp['Date']);df_AMZN_comp.set_axis(df_AMZN_comp['Date'], inplace=True);\n",
    "# df_BTC_comp['Date'] = pd.to_datetime(df_BTC_comp['Date']);df_BTC_comp.set_axis(df_BTC_comp['Date'], inplace=True);\n",
    "# df_FDX_comp['Date'] = pd.to_datetime(df_FDX_comp['Date']);df_FDX_comp.set_axis(df_FDX_comp['Date'], inplace=True);\n",
    "# df_IBM_comp['Date'] = pd.to_datetime(df_IBM_comp['Date']);df_IBM_comp.set_axis(df_IBM_comp['Date'], inplace=True);\n",
    "# df_MSFT_comp['Date'] = pd.to_datetime(df_MSFT_comp['Date']);df_MSFT_comp.set_axis(df_MSFT_comp['Date'], inplace=True);\n",
    "# df_NVDA_comp['Date'] = pd.to_datetime(df_NVDA_comp['Date']);df_NVDA_comp.set_axis(df_NVDA_comp['Date'], inplace=True);\n",
    "# df_ORCL_comp['Date'] = pd.to_datetime(df_ORCL_comp['Date']);df_ORCL_comp.set_axis(df_ORCL_comp['Date'], inplace=True);\n",
    "# df_GOOG_comp['Date'] = pd.to_datetime(df_GOOG_comp['Date']);df_GOOG_comp.set_axis(df_GOOG_comp['Date'], inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col(df):\n",
    "    df = df.drop(columns=['Open', 'High', 'Low', 'Volume'], inplace=True);\n",
    "    return df;\n",
    "\n",
    "drop_col(df_TSLA)\n",
    "drop_col(df_AAPL)\n",
    "drop_col(df_ABNB)\n",
    "drop_col(df_AMZN)\n",
    "drop_col(df_BTC)\n",
    "drop_col(df_FDX)\n",
    "drop_col(df_IBM)\n",
    "drop_col(df_MSFT)\n",
    "drop_col(df_NVDA)\n",
    "drop_col(df_ORCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_federated'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emili\\Downloads\\Machine_Learning_SP_Course\\Machine_Learning_Project\\Federated_Learning.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emili/Downloads/Machine_Learning_SP_Course/Machine_Learning_Project/Federated_Learning.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emili/Downloads/Machine_Learning_SP_Course/Machine_Learning_Project/Federated_Learning.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/emili/Downloads/Machine_Learning_SP_Course/Machine_Learning_Project/Federated_Learning.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_federated\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtff\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emili/Downloads/Machine_Learning_SP_Course/Machine_Learning_Project/Federated_Learning.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dataframes \u001b[39m=\u001b[39m [df_TSLA, df_AAPL, df_ABNB, df_AMZN, df_BTC, df_FDX, df_IBM, df_MSFT, df_NVDA, df_ORCL]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emili/Downloads/Machine_Learning_SP_Course/Machine_Learning_Project/Federated_Learning.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m clients_data \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dataframes]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_federated'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "dataframes = [df_TSLA, df_AAPL, df_ABNB, df_AMZN, df_BTC, df_FDX, df_IBM, df_MSFT, df_NVDA, df_ORCL]\n",
    "\n",
    "clients_data = [df.values for df in dataframes]\n",
    "\n",
    "# Number of Clients\n",
    "n = 5\n",
    "clients_data = np.array_split(close_data, n)\n",
    "\n",
    "# Define preprocessing for your data\n",
    "def preprocess(dataset):\n",
    "    def batch_format_fn(element):\n",
    "        return collections.OrderedDict(\n",
    "            x=tf.reshape(element, [-1, your_input_shape]),\n",
    "            y=tf.reshape(element, [-1, 1])\n",
    "        )\n",
    "    return dataset.batch(1).map(batch_format_fn).prefetch(1)\n",
    "\n",
    "# Prepare federated data\n",
    "federated_train_data = [preprocess(client_data) for client_data in clients_data]\n",
    "\n",
    "# Define the model function compatible with TFF\n",
    "def model_fn():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(30, activation='LeakyReLU', input_shape=(look_back, your_input_shape)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return tff.learning.from_keras_model(\n",
    "        model,\n",
    "        input_spec=federated_train_data[0].element_spec,\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    "    )\n",
    "\n",
    "# Build Federated Averaging Process\n",
    "federated_averaging_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam()\n",
    ")\n",
    "\n",
    "# Initialize the Federated Averaging Process\n",
    "state = federated_averaging_process.initialize()\n",
    "\n",
    "# Run the Federated Learning\n",
    "NUM_ROUNDS = 10  # For example\n",
    "for round_num in range(NUM_ROUNDS):\n",
    "    state, metrics = federated_averaging_process.next(state, federated_train_data)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_federated\n",
      "  Using cached tensorflow_federated-0.33.0-py2.py3-none-any.whl (885 kB)\n",
      "Collecting absl-py~=1.0.0 (from tensorflow_federated)\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting attrs~=21.4.0 (from tensorflow_federated)\n",
      "  Using cached attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting cachetools~=3.1.1 (from tensorflow_federated)\n",
      "  Using cached cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting dm-tree~=0.1.7 (from tensorflow_federated)\n",
      "  Using cached dm_tree-0.1.8-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Collecting farmhashpy~=0.4.0 (from tensorflow_federated)\n",
      "  Using cached farmhashpy-0.4.0.tar.gz (98 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting grpcio~=1.46.3 (from tensorflow_federated)\n",
      "  Using cached grpcio-1.46.5.tar.gz (21.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of tensorflow-federated to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow_federated\n",
      "  Using cached tensorflow_federated-0.32.0-py2.py3-none-any.whl (884 kB)\n",
      "  Using cached tensorflow_federated-0.31.0-py2.py3-none-any.whl (884 kB)\n",
      "  Using cached tensorflow_federated-0.30.0-py2.py3-none-any.whl (875 kB)\n",
      "Collecting attrs~=21.2.0 (from tensorflow_federated)\n",
      "  Using cached attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting tensorflow_federated\n",
      "  Using cached tensorflow_federated-0.29.0-py2.py3-none-any.whl (867 kB)\n",
      "  Using cached tensorflow_federated-0.28.0-py2.py3-none-any.whl (859 kB)\n",
      "  Using cached tensorflow_federated-0.27.0-py2.py3-none-any.whl (861 kB)\n",
      "Collecting grpcio~=1.34.0 (from tensorflow_federated)\n",
      "  Using cached grpcio-1.34.1.tar.gz (21.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jax~=0.2.27 (from tensorflow_federated)\n",
      "  Using cached jax-0.2.28.tar.gz (887 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorflow_federated\n",
      "  Using cached tensorflow_federated-0.26.0-py2.py3-none-any.whl (862 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflow-federated to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached tensorflow_federated-0.24.0-py2.py3-none-any.whl (866 kB)\n",
      "  Using cached tensorflow_federated-0.23.0-py2.py3-none-any.whl (866 kB)\n",
      "  Using cached tensorflow_federated-0.22.0-py2.py3-none-any.whl (863 kB)\n",
      "  Using cached tensorflow_federated-0.21.0-py2.py3-none-any.whl (857 kB)\n",
      "  Using cached tensorflow_federated-0.20.0-py2.py3-none-any.whl (819 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached tensorflow_federated-0.19.0-py2.py3-none-any.whl (602 kB)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\emili\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_federated) (0.15.0)\n",
      "Collecting attrs~=19.3.0 (from tensorflow_federated)\n",
      "  Using cached attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting tensorflow_federated\n",
      "  Using cached tensorflow_federated-0.18.0-py2.py3-none-any.whl (578 kB)\n",
      "Collecting grpcio~=1.32.0 (from tensorflow_federated)\n",
      "  Using cached grpcio-1.32.0.tar.gz (20.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting h5py~=2.10.0 (from tensorflow_federated)\n",
      "  Using cached h5py-2.10.0.tar.gz (301 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorflow_federated\n",
      "  Using cached tensorflow_federated-0.17.0-py2.py3-none-any.whl (517 kB)\n",
      "Collecting absl-py~=0.9.0 (from tensorflow_federated)\n",
      "  Using cached absl-py-0.9.0.tar.gz (104 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\emili\\AppData\\Local\\Temp\\pip-install-l1phpl67\\absl-py_67471ea40c5d48fbaef4ce468bfd3b38\\setup.py\", line 34, in <module>\n",
      "          raise RuntimeError('Python version 2.7 or 3.4+ is required.')\n",
      "      RuntimeError: Python version 2.7 or 3.4+ is required.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(df,df_future,df_comp):\n",
    "    close_data = df['Close'].values\n",
    "    close_data = close_data.reshape((-1,1));\n",
    "\n",
    "    split_percent = 0.80;\n",
    "    split = int(split_percent*len(close_data));\n",
    "\n",
    "    close_train = close_data[:split];\n",
    "    close_test = close_data[split:];\n",
    "\n",
    "    date_train = df['Date'][:split];\n",
    "    date_test = df['Date'][split:];\n",
    "\n",
    "    #print(len(close_train));\n",
    "    #print(len(close_test));\n",
    "    ## You should get that the train and testing sets have been split \n",
    "    ## into 80% train and 20% test\n",
    "\n",
    "    look_back = 5;\n",
    "\n",
    "    train_generator = TimeseriesGenerator(close_train, close_train, length = look_back, batch_size = 200);\n",
    "    test_generator = TimeseriesGenerator(close_test, close_test, length = look_back, batch_size = 200);\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(30,\n",
    "            activation='LeakyReLU',\n",
    "            input_shape=(look_back,1))\n",
    "    )\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    num_epochs = 100;\n",
    "    model.fit(train_generator, epochs=num_epochs, verbose=1)\n",
    "    prediction = model.predict(test_generator)\n",
    "\n",
    "    close_train = close_train.reshape((-1))\n",
    "    close_test = close_test.reshape((-1))\n",
    "    prediction = prediction.reshape((-1))\n",
    "\n",
    "    close_data = close_data.reshape((-1));\n",
    "\n",
    "    def predict(num_prediction, model):\n",
    "        prediction_list = close_data[-look_back:];\n",
    "    \n",
    "        for _ in range(num_prediction):\n",
    "            x = prediction_list[-look_back:];\n",
    "            x = x.reshape((1, look_back, 1));\n",
    "            out = model.predict(x)[0][0];\n",
    "            prediction_list = np.append(prediction_list, out);\n",
    "        prediction_list = prediction_list[look_back-1:];\n",
    "        \n",
    "        return prediction_list;\n",
    "    \n",
    "    def predict_dates(num_prediction):\n",
    "        last_date = df['Date'].values[-1];\n",
    "        prediction_dates = pd.date_range(last_date, periods=num_prediction+1).tolist();\n",
    "        return prediction_dates;\n",
    "\n",
    "    num_prediction = 29;\n",
    "    forecast = predict(num_prediction, model);\n",
    "    forecast_dates = predict_dates(num_prediction);\n",
    "    trace1 = go.Scatter(\n",
    "        x = date_train,\n",
    "        y = close_train,\n",
    "        mode = 'lines',\n",
    "        name = 'Training Data'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = date_test,\n",
    "        y = prediction,\n",
    "        mode = 'lines',\n",
    "        name = 'Testing Training Data'\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x = date_test,\n",
    "        y = close_test,\n",
    "        mode='lines',\n",
    "        name = 'Ground Truth'\n",
    "    )\n",
    "    if (df is df_TSLA):\n",
    "        name = \"TSLA\"\n",
    "    elif(df is df_AAPL):\n",
    "        name = \"AAPL\"\n",
    "    elif(df is df_ABNB):\n",
    "        name = \"ABNB\"\n",
    "    elif(df is df_AMZN):\n",
    "        name = \"AMZN\"\n",
    "    elif(df is df_BTC):\n",
    "        name = \"BTC\"\n",
    "    elif(df is df_FDX):\n",
    "        name = \"FDX\"\n",
    "    elif(df is df_IBM):\n",
    "        name = \"IBM\"\n",
    "    elif(df is df_MSFT):\n",
    "        name = \"MSFT\"\n",
    "    elif(df is df_NVDA):\n",
    "        name = \"NVDA\"\n",
    "    elif(df is df_ORCL):\n",
    "        name = \"ORCL\"\n",
    "    elif(df is df_GOOG):\n",
    "        name = \"GOOG\"\n",
    "    else:\n",
    "        name = \"\"\n",
    "    layout = go.Layout(\n",
    "        title = (name + \" Stock\"),\n",
    "        font = dict(size = 20),\n",
    "        xaxis = {'title' : \"Date\"},\n",
    "        yaxis = {'title' : \"Close\"}\n",
    "    )\n",
    "    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n",
    "    fig.show()\n",
    "    test_r2_score = r2_score(close_test[0:len(close_test)-look_back],prediction)\n",
    "    print(\"The R2_score of this Training-Testing model is:\",test_r2_score)\n",
    "    trace1 = go.Scatter(\n",
    "        x = forecast_dates,\n",
    "        y = forecast,\n",
    "        mode = 'lines',\n",
    "        name = 'Future Predictions'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = df_comp.index,\n",
    "        y = df_comp['Close'],\n",
    "        mode = 'lines',\n",
    "        name = '30-Day Data'\n",
    "    )\n",
    "    layout = go.Layout(\n",
    "        title = (name + \" Stock\"),\n",
    "        font = dict(size = 20),\n",
    "        xaxis = {'title' : \"Date\"},\n",
    "        yaxis = {'title' : \"Close\"}\n",
    "    )\n",
    "    fig = go.Figure(data=[trace1,trace2, trace3], layout=layout);\n",
    "    fig.show();\n",
    "    df_r2_score = r2_score(df_comp['Close'], forecast)\n",
    "    print(\"The R2_score of this prediction model is:\",df_r2_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

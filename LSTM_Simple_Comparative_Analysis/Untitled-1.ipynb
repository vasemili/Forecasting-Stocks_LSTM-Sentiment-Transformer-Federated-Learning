{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'producer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39muuid\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mproducer\u001b[39;00m \u001b[39mimport\u001b[39;00m KafkaJsonProducer\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'producer'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import string\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from producer import KafkaJsonProducer\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['Kafka_host'] = \"url:9902\"\n",
    "os.environ['Cluster_key'] = 'key_num'\n",
    "os.environ['Cluster_secret'] = 'secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    def __init__(self, json_data):\n",
    "        self.data = json_data\n",
    "\n",
    "    def update_fields(self, first_name, cx_status, cx_substatus, loan_number):\n",
    "        self.update_variable('FirstName', first_name)\n",
    "\n",
    "        def update_numeric_value(data):\n",
    "            if isinstance(data, dict):\n",
    "                if 'fieldName' in data and data['fieldName'] == 'CX.STATUS':\n",
    "                    if 'numericValue' in data:\n",
    "                        data['numericValue'] = cx_status\n",
    "\n",
    "                for value in data.values():\n",
    "                    if isinstance(value, (dict, list)):\n",
    "                        update_numeric_value(value)\n",
    "\n",
    "        update_numeric_value(self.data)\n",
    "\n",
    "        self.update_variable('CX.SUBSTATUS', cx_substatus)\n",
    "        self.update_variable('encompassId', str(uuid.uuid4()))\n",
    "        self.update_variable('loanNumber', loan_number)\n",
    "\n",
    "    def update_variable(self, variable_name, new_value, data=None):\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key == variable_name:\n",
    "                    data[key] = new_value\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    self.update_variable(variable_name, new_value, value)\n",
    "\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                self.update_variable(variable_name, new_value, item)\n",
    "\n",
    "\n",
    "class JsonProducer:\n",
    "    def __init__(self):\n",
    "        self.json_producer = KafkaJsonProducer(producer_client_id='python-sample')\n",
    "\n",
    "    def produce_json_message(self, data_class):\n",
    "        key = str(uuid.uuid4())\n",
    "        value = data_class.data\n",
    "        self.json_producer.send(topic_name='example', key=key, value=value)\n",
    "\n",
    "\n",
    "json_producer = JsonProducer()\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = 'path_to_your_json_file.json'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate 500 unique messages with random values for \"CX.STATUS\", \"CX.SUBSTATUS\", \"encompassId\", and \"loanNumber\"\n",
    "for _ in range(500):\n",
    "    first_name = ''.join(random.choices(string.ascii_letters, k=8))\n",
    "    cx_status = random.randint(1, 9)\n",
    "    cx_substatus = random.choice(string.ascii_uppercase)\n",
    "    loan_number = str(random.randint(10**8, 10**9-1))\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    data = DataClass(json_data)\n",
    "    data.update_fields(first_name, cx_status, cx_substatus, loan_number)\n",
    "\n",
    "    json_producer.produce_json_message(data)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Get the desired values\n",
    "min_insync_replicas = os.environ.get('min_insync_replicas')\n",
    "cleanup_policy = os.environ.get('cleanup_policy')\n",
    "retention_ms = os.environ.get('retention_ms')\n",
    "max_message_bytes = os.environ.get('max_message_bytes')\n",
    "\n",
    "print(f\"min_insync_replicas: {min_insync_replicas}\")\n",
    "print(f\"cleanup_policy: {cleanup_policy}\")\n",
    "print(f\"retention_ms: {retention_ms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.cimpl import TopicPartition\n",
    "\n",
    "from mrckafka.commons.exceptions.MrcKafkaErrors import MrcKafkaSerializationException, MrcKafkaProducerException\n",
    "from .KafkaProducer import KafkaProducer\n",
    "import logging\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "class KafkaJsonProducer(KafkaProducer):\n",
    "    def __init__(self, producer_client_id):\n",
    "        self.err = None\n",
    "        self.topic_partition = None\n",
    "        super(KafkaJsonProducer, self).__init__(producer_client_id = producer_client_id)\n",
    "        \n",
    "        json_producer_logger = logging.getLogger('json_producer')\n",
    "        json_producer_logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        logging.info(\"Setting up Kafka Json Producer\")\n",
    "        self.json_producer = Producer(self.producer_conf, logger = json_producer_logger)\n",
    "        logging.info(\"kafka Json Producer Configured\")\n",
    "        \n",
    "    def delivery_report(self, err, msg):\n",
    "        \"\"\"\n",
    "        \n",
    "        reports the failure or success of a message delivery\n",
    "        Args:\n",
    "            err (KafkaError): The error that occurred on None on success.\n",
    "            msg (Message): The message that was produced or failed.\n",
    "        \"\"\"\n",
    "        if err is not None:\n",
    "            logging.error(\"Delivery failed for User record {}: {}\".format{msg.key(), err})\n",
    "            self.err = err\n",
    "        else:\n",
    "            logging.info(\n",
    "                'Record successfully delivered -> TOPIC: {} - Partition [{}] - offset {}'.format(\n",
    "                    msg.topic(), msg.partition(), msg.offset()))\n",
    "            self.topic_partition = TopicPartition(topic = msg.topic(), partition = msg.partition(), offset = msg.offset())\n",
    "            \n",
    "        \n",
    "    def validatePayload(self, topic_name, key, value):\n",
    "        if not topic_name:\n",
    "            raise MrcKafkaProducerException(message = \"Topic Name cannot be null or empty\")\n",
    "        if not key:\n",
    "            raise MrcKafkaProducerException(message = \"Key cannot be null or empty\")\n",
    "        if not value:\n",
    "            raise MrcKafkaProducerException(message = \"Value cannot be null or empty\")\n",
    "    \n",
    "    def send(self, topic_name, key, value, headers = None):\n",
    "        self.validatePayload(topic_name,key, value)\n",
    "        \n",
    "        ##Convert to JSON String\n",
    "        try:\n",
    "            jsonValue = json.dumps(value.__dict__)\n",
    "        except AttributeError as err:\n",
    "            raise MrcKafkaSerializationException(\"Invalid class object sent. Please send a class. \\n {}\".format(err))\n",
    "        \n",
    "        self.json_producer.produce(topic = topic_name, key = key, value = jsonValue, headers = headers, on_delivery = (lambda err, msg: self.delivery_report(err,msg)))\n",
    "        \n",
    "        self.json_producer.flush()\n",
    "        if self.err is None:\n",
    "            traceId = None\n",
    "            if headers is not None:\n",
    "                if type(headers) is dict:\n",
    "                    traceId = headers.get('traceId', None)\n",
    "            if traceId is not None:\n",
    "                logging.info('TraceId: {} | Message sent to topic {}'.format(traceId, topic_name))\n",
    "            return self.topic_partition\n",
    "        else:\n",
    "            raise MrcKafkaProducerException(error = self.err, message = self.err.str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from confluent_kafka import Consumer\n",
    "import logging\n",
    "\n",
    "from confluent_kafka.serialization import StringDeserializer\n",
    "\n",
    "from mrckafka.commons.exceptions.MrcKafkaErrors import MrcKafkaConsumerException, MrcKafkaDeserializationException\n",
    "\n",
    "from .ByteProducerFactory import ByteProducerFactory\n",
    "from .KafkaConsumer import KafkaConsumer\n",
    "\n",
    "from .utils.CommitCode import CommitCode\n",
    "from .utils.CommitResult import CommitResult\n",
    "from .utils.ConsumerRecord import ConsumerRecord\n",
    "from .utils.JsonDeserializer import JsonDeserializier\n",
    "\n",
    "class KafkaJsonConsumer(KafkaConsumer):\n",
    "    def __init__(self,record_class, consumer_group_id):\n",
    "        super(KafkaJsonConsumer,self).__init__(consumer_group_id)\n",
    "        \n",
    "        json_consumer_logger = logging.getLogger('json_consumer')\n",
    "        json_consumer_logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        logging.info('Setting up Json Consumer')\n",
    "        self.json_consumer = Consumer(self.consumer_conf,logger =json_consumer_logger)\n",
    "        \n",
    "        self._key_deserializer = StringDeserializer('utf-8')\n",
    "        self._value_deserializer = JsonDeserializier(record_class)\n",
    "        self._byte_product_factory = ByteProducerFactory()\n",
    "        self._consumer_group_id = consumer_group_id\n",
    "        \n",
    "        self._retry_counter = 0\n",
    "        self._max_retries = self.config.max_retries\n",
    "        self._retry_interval = (self.config.retry_interval) / 1000.0\n",
    "        \n",
    "    def message_retry_handler(self, consumer_record,retry_topic, commit_result):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "from confluent_kafka import Producer\n",
    "import uuid\n",
    "\n",
    "class DataClass:\n",
    "    def __init__(self, json_data):\n",
    "        self.data = json_data\n",
    "\n",
    "    def update_fields(self, first_name, cx_status, cx_substatus, loan_number):\n",
    "        self.update_variable('FirstName', first_name)\n",
    "\n",
    "        def update_numeric_value(data):\n",
    "            if isinstance(data, dict):\n",
    "                if 'fieldName' in data and data['fieldName'] == 'CX.STATUS':\n",
    "                    if 'numericValue' in data:\n",
    "                        data['numericValue'] = cx_status\n",
    "\n",
    "                for value in data.values():\n",
    "                    if isinstance(value, (dict, list)):\n",
    "                        update_numeric_value(value)\n",
    "\n",
    "        update_numeric_value(self.data)\n",
    "\n",
    "        self.update_variable('CX.SUBSTATUS', cx_substatus)\n",
    "        self.update_variable('encompassId', str(uuid.uuid4()))\n",
    "        self.update_variable('loanNumber', loan_number)\n",
    "\n",
    "    def update_variable(self, variable_name, new_value, data=None):\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key == variable_name:\n",
    "                    data[key] = new_value\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    self.update_variable(variable_name, new_value, value)\n",
    "\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                self.update_variable(variable_name, new_value, item)\n",
    "\n",
    "class JsonProducer:\n",
    "    def __init__(self):\n",
    "        self.json_producer = KafkaJsonProducer(producer_client_id='python-sample')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def produce_json_message(self, data_class):\n",
    "        key = str(uuid.uuid4())\n",
    "        value = data_class.data\n",
    "        self.json_producer.send(topic_name='example', key=key, value=value)\n",
    "        self.logger.info('Produced a message')  # Log the message production\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = 'path_to_your_json_file.json'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configure the logger\n",
    "logging.basicConfig(filename='producer.log', level=logging.INFO)\n",
    "\n",
    "# Generate 500 unique messages with random values for \"CX.STATUS\", \"CX.SUBSTATUS\", \"encompassId\", and \"loanNumber\"\n",
    "for _ in range(500):\n",
    "    first_name = ''.join(random.choices(string.ascii_letters, k=8))\n",
    "    cx_status = random.randint(1, 9)\n",
    "    cx_substatus = random.choice(string.ascii_uppercase)\n",
    "    loan_number = str(random.randint(10**8, 10**9-1))\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    data = DataClass(json_data)\n",
    "    data.update_fields(first_name, cx_status, cx_substatus, loan_number)\n",
    "\n",
    "    json_producer.produce_json_message(data)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Get the desired values\n",
    "min_insync_replicas = os.environ.get('min_insync_replicas')\n",
    "cleanup_policy = os.environ.get('cleanup_policy')\n",
    "retention_ms = os.environ.get('retention_ms')\n",
    "max_message_bytes = os.environ.get('max_message_bytes')\n",
    "\n",
    "print(f\"min_insync_replicas: {min_insync_replicas}\")\n",
    "print(f\"cleanup_policy: {cleanup_policy}\")\n",
    "print(f\"retention_ms: {retention_ms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Slf4j\n",
    "@Component\n",
    "public class StreamProcessor extends MRCKafkaStream{\n",
    "    RestTemplates restTemplate = new RestTemplate();\n",
    "    \n",
    "    private static final String LEFT = \"STREAMING.SAMPLE-APPLICATION.STRING-TOPIC\";\n",
    "    private static final String RIGHT = \"STREAMING.SAMPLE-APPLICATION.JSON-TOPIC\";\n",
    "    private static final String OUT = \"STREAMING.SAMPLE-APPLICATION.AVRO-TOPIC\";\n",
    "    \n",
    "    private void employeeStreamProcessingLogic(StreamsBuilder streamsBuilder){\n",
    "        Ktable<String, String> right = streamsBuilder.table(\n",
    "            RIGHT,\n",
    "            consumeWith(\n",
    "                ContentType.STRING, String.class,\n",
    "                ContentType.STRING, String.class\n",
    "            ),\n",
    "            Materialized.as(\"store-name\")\n",
    "        );\n",
    "        KStream<String, String> left = streamsbuilder.stream(\n",
    "            LEFT,\n",
    "            consumedWith(\n",
    "                ContentType.STRING, String.class,\n",
    "                ContentType.STRING, String.class\n",
    "            ))\n",
    "                .join(right, (leftObject, rightObject) -> leftObject+rightObject,\n",
    "                      Joined.with(\n",
    "                          getSerde(ContentType.STRING, String.class, true),\n",
    "                          getSerde(ContentType.STRING, String.class, false),\n",
    "                          getSerde(ContentType.STRING, String.class, false)\n",
    "                      ));\n",
    "        streamTo(OUT, left, ContentType.STRING, String.class, ContentType.STRING, String.class);\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    public void defineStreamProcessingLogic(SteamsBuilder streamsBuilder) {\n",
    "        employeeStreamProcessingLogic(streamsBuilder)\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
